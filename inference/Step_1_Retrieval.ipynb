{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import util\n",
    "from functools import reduce\n",
    "from underthesea import sent_tokenize\n",
    "from tqdm.notebook import tqdm\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_NAME = \"keepitreal/vietnamese-sbert\"\n",
    "MODEL_NAME = \"Oztobuzz/sbert_mnr_5_epoch_v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(MODEL_NAME).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = '../data/private/ise-dsc01-private-test-offcial.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_data = json.load(open(INPUT_DIR))\n",
    "data = [(dict_data[key], key) for key in dict_data.keys()]\n",
    "index = [x[1] for x in data]\n",
    "context = [x[0]['context'] for x in data]\n",
    "claim = [x[0]['claim'] for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_claim(claim):\n",
    "    \"\"\"\n",
    "    A function to create embedding for claims\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    claim: List[str]\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    np.array\n",
    "    \"\"\"\n",
    "    return model.encode(claim, batch_size=64, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_context(context, split_type):\n",
    "    \"\"\"\n",
    "    A function to preprocess context\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    context: List[str]\n",
    "    split_type: PARAGRAPH or SENTENCE\n",
    "        Use 'PARAGRAPH' to split the context with \\n\\n\n",
    "        Use 'SENTENCE' to split the context with .\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    List[List[str]]\n",
    "    \"\"\"\n",
    "    if split_type == 'PARAGRAPH':\n",
    "        return [x.split('\\n\\n') for x in context]\n",
    "    elif split_type == 'SENTENCE':\n",
    "        context = [sent_tokenize(x) for x in context]\n",
    "        context = [[re.split(r'[.][.]+',x) for x in y] for y in context]\n",
    "        context = [list(itertools.chain(*x)) for x in context]\n",
    "        return context\n",
    "    else:\n",
    "        raise Exception(\"Please use PARAGRAPH or SENTENCE for split_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_context(context):\n",
    "    \"\"\"\n",
    "    A function to create embedding of context or eidence\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    context: List[List[str]]\n",
    "        List of context\n",
    "    Returns\n",
    "    ---------\n",
    "    context_embeddng: List[np.array()] with dim = (number of paragraph/array in each context, embedding size)\n",
    "    \"\"\"\n",
    "    # Create length list\n",
    "    context_length = [len(x) for x in context]\n",
    "    \n",
    "    # Embed flatten context\n",
    "    flatten_context_embedding = model.encode(\n",
    "        list(itertools.chain(*context)),\n",
    "        batch_size=64,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    \n",
    "    # Remap context\n",
    "    context_embedding = list()\n",
    "    start_index = 0\n",
    "    for length in context_length:\n",
    "        context_embedding.append(flatten_context_embedding[start_index:start_index+length])\n",
    "        start_index += length\n",
    "    \n",
    "    return context_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(claim_embedding, context_embedding, context, top_k=1, threshold=None):\n",
    "    \"\"\"\n",
    "    A function to retrieve relevant contexts with respect to claims\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    claim: List[str]\n",
    "        List of claims\n",
    "    context: List[np.array()]\n",
    "        List of context embedding\n",
    "    top_k: int\n",
    "        Top result to return\n",
    "    threshold: float\n",
    "        Threshold value to get\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    retrieve_result: List[Dict]\n",
    "    \"\"\"\n",
    "    \n",
    "    retrieve_result = list()\n",
    "    for i in tqdm(range(len(claim))):\n",
    "        hits = util.semantic_search(claim_embedding[i], context_embedding[i], top_k=top_k)\n",
    "        hits = hits[0]\n",
    "        temp=list()\n",
    "        for hit in hits:\n",
    "            if (threshold==None) or (threshold and hit['score']>=threshold):\n",
    "                row_evidence = context[i][hit['corpus_id']]\n",
    "                if len(row_evidence) > 10:\n",
    "                    temp.append({'evidence': row_evidence, 'score': round(hit['score'],4), 'id': hit['corpus_id']})\n",
    "        if len(temp) == 0:\n",
    "            print(\"empty case\")\n",
    "            temp.append({'evidence': '', 'score': 1.0, 'id': -1})\n",
    "        retrieve_result.append(temp)\n",
    "    return retrieve_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_comma(text):\n",
    "    \"\"\"\n",
    "    A function to add comma at the end of a sentence\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text: str\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    str\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if text.strip()[-1] != '.':\n",
    "            text+='.'\n",
    "        return text\n",
    "    except:\n",
    "        print(text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_embedding = embed_claim(\n",
    "    claim=claim\n",
    ")\n",
    "\n",
    "# Get Top 5 Paragraphs\n",
    "\n",
    "paragraph = split_context(\n",
    "    context=context,\n",
    "    split_type='PARAGRAPH'\n",
    ")\n",
    "\n",
    "context_embedding = embed_context(\n",
    "    context=paragraph\n",
    ")\n",
    "\n",
    "\n",
    "paragraph_result = retrieve(\n",
    "    claim_embedding=claim_embedding,\n",
    "    context_embedding=context_embedding,\n",
    "    context=paragraph,\n",
    "    top_k=5,\n",
    "    threshold=None,\n",
    ")\n",
    "\n",
    "# Get Top 5 Sentences\n",
    "\n",
    "sentence = split_context(\n",
    "    context=[\" \".join([add_comma(x['evidence']) for x in y]) for y in paragraph_result],\n",
    "    split_type='SENTENCE'\n",
    ")\n",
    "\n",
    "sentence_embedding = embed_context(\n",
    "    context=sentence\n",
    ")\n",
    "\n",
    "sentence_result = retrieve(\n",
    "    claim_embedding=claim_embedding,\n",
    "    context_embedding=sentence_embedding,\n",
    "    context=sentence,\n",
    "    top_k=5,\n",
    "    threshold=None,\n",
    ")\n",
    "\n",
    "# Append to original dict\n",
    "retrieve_result = json.load(open(INPUT_DIR))\n",
    "for sample_order, sample_number in enumerate(index):\n",
    "    retrieve_result[sample_number]['evidence'] = [x['evidence'] for x in sentence_result[sample_order]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save New Result\n",
    "with open(\"../result/retrieve_result/private_test_retrieval_v1_top5_top_5.json\", \"w\") as outfile:\n",
    "    json.dump(retrieve_result, outfile)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
