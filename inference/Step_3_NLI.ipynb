{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "new_directory = \"drive/MyDrive/02_Project/UIT_Fact_Checking_Contest/FNU/\" # Change current directory into MainTraining in Google Drive\n",
    "os.chdir(new_directory)\n",
    "print(\"Current Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import T5EncoderModel\n",
    "from transformers import RobertaModel\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics._classification import _check_targets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import zipfile\n",
    "import json\n",
    "import datasets\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envir = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') # Check again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(text):\n",
    "    text = re.sub(r\"\\n\", \"\", text)\n",
    "    text = text.strip()\n",
    "    text = text[1:].strip() if text[0]=='.' else text\n",
    "    text = text[:-1].strip() if text[-1]=='.' else text\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "def mDeBERTa_tokenize(data): # mDeBERTa model: Taking input_ids\n",
    "    if CLEAN_DATA == True:\n",
    "        premises = [data_cleaning(premise) for premise, _ in data['sample']]\n",
    "        hypothesis = [data_cleaning(hypothesis) for _, hypothesis in data['sample']]\n",
    "    else:\n",
    "        premises = [premise for premise, _ in data['sample']]\n",
    "        hypothesis = [hypothesis for _, hypothesis in data['sample']]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input_token = (tokenizer(premises, hypothesis, truncation=True, return_tensors=\"pt\", padding = True)['input_ids']).to(envir)\n",
    "        embedding = model(input_token).last_hidden_state\n",
    "\n",
    "    mean_embedding = torch.mean(embedding[:, 1:, :], dim = 1)\n",
    "    cls_embedding = embedding[:, 0, :]\n",
    "\n",
    "    return {'mean':mean_embedding, 'cls':cls_embedding}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLI_model(nn.Module):\n",
    "    def __init__(self, input_dims, class_weights=torch.tensor([0., 0., 0.])):\n",
    "        super(NLI_model, self).__init__()\n",
    "\n",
    "        self.classification = nn.Sequential(\n",
    "            nn.Linear(input_dims, 3)\n",
    "        )\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss(class_weights)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output_linear = self.classification(input)\n",
    "        return output_linear\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx=0):\n",
    "        input_data, targets = train_batch\n",
    "        outputs = self.forward(input_data)\n",
    "        loss = self.criterion(outputs, targets)\n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, batch, batch_idx=0):\n",
    "        input_data, _ = batch\n",
    "        outputs = self.forward(input_data)\n",
    "        prob = outputs.softmax(dim = -1)\n",
    "        sort_prob, sort_indices = torch.sort(-prob, 1)\n",
    "        return sort_indices[:,0], sort_prob[:,0]\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx=0):\n",
    "        _, targets = val_batch\n",
    "        sort_indices, _ = self.predict_step(val_batch, batch_idx)\n",
    "        report = classification_report(list(targets.to('cpu').numpy()), list(sort_indices.to('cpu').numpy()), output_dict=True, zero_division = 1)\n",
    "        return report\n",
    "\n",
    "    def test_step(self, batch, dict_form, batch_idx=0):\n",
    "        _, targets = batch\n",
    "        sort_indices, _ = self.predict_step(batch, batch_idx)\n",
    "        report = classification_report(targets.to('cpu').numpy(), sort_indices.to('cpu').numpy(), output_dict=dict_form, zero_division = 1)\n",
    "        return report\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = \"Input\"\n",
    "file_name = \"private_test_retrieval_v1_top5_top_5.json\"\n",
    "\n",
    "result_folder = \"05_NLI\"\n",
    "\n",
    "if not(os.path.exists(result_folder) and os.path.isdir(result_folder)):\n",
    "    os.mkdir(result_folder)\n",
    "    print(\"Created directory:\", result_folder)\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "int2label = {0:'SUPPORTED', 1:'NEI', 2:'REFUTED'}\n",
    "\n",
    "folder = \"mDeBERTa (ft) V6\"\n",
    "\n",
    "if folder == 'mDeBERTa (ft) V6': CLEAN_DATA = True\n",
    "else: CLEAN_DATA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_json(f\"{input_folder}/{file_name}\")\n",
    "test_set = {'sample':[], 'key':[], 'subkey':[]}\n",
    "for key, values in dataframe.items():\n",
    "    if len(values['evidence']) > 0:\n",
    "        for subkey, evidence in enumerate(values['evidence']):\n",
    "            test_set['sample'].append( (evidence, values['claim']) )\n",
    "            test_set['key'].append(key)\n",
    "            test_set['subkey'].append(subkey)\n",
    "    else:\n",
    "        raise Exception(\"Do not have evidence !!!\")\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'test':Dataset.from_dict(test_set)\n",
    "})\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_mapping(batch):\n",
    "    with torch.no_grad():\n",
    "        predict_label, predict_prob = model.predict_step((batch[input_type].to(envir), None))\n",
    "    return {'label':predict_label, 'prob':-predict_prob}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_predictedDataset(predict_dataset, origin_dataframe):\n",
    "    data_frame = {}\n",
    "    for key, value in origin_dataframe.items():\n",
    "        data_frame[key] = {}\n",
    "        for field, contend in value.items():\n",
    "            data_frame[key][field] = contend\n",
    "        data_frame[key]['verdict'] = ['' for i in range(len(data_frame[key]['evidence']))]\n",
    "        data_frame[key]['prob'] = [1 for i in range(len(data_frame[key]['verdict']))]\n",
    "\n",
    "    for record in predict_dataset:\n",
    "        data_frame[ record['key'].item() ]['verdict'][ record['subkey'].item() ] = int2label[ record['label'].item() ]\n",
    "        data_frame[ record['key'].item() ]['prob'][ record['subkey'].item() ] = record['prob'].item()\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in [\"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\"]:\n",
    "    merge_dataset = DatasetDict({\"infer\": datasets.concatenate_datasets([dataset['test']])})\n",
    "\n",
    "    for input_type in ['cls', 'mean']:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModel.from_pretrained(f\"{folder}/{(model_name.split('/'))[-1]}-{input_type}\").to(envir)\n",
    "        merge_dataset = merge_dataset.map(mDeBERTa_tokenize, batched=True, batch_size=BATCH_SIZE)\n",
    "        merge_dataset = merge_dataset.with_format(\"torch\", [input_type, 'key', 'subkey'])\n",
    "\n",
    "    # Load classifier model\n",
    "        checkpoints = torch.load(f\"{folder}/{input_type}.pt\", map_location = envir)\n",
    "        model = NLI_model(merge_dataset['infer'][input_type].shape[-1], torch.tensor([0., 0., 0.])).to(envir)\n",
    "        model.load_state_dict(checkpoints['model_state_dict'])\n",
    "\n",
    "    # Runing Inference step\n",
    "        predicted_dataset = merge_dataset.map(predict_mapping, batched=True, batch_size=merge_dataset['infer'].num_rows)\n",
    "    # Outputing the dataset\n",
    "        new_dataframe = output_predictedDataset(predicted_dataset['infer'], dataframe)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{result_folder}/{file_name.replace('.json', '')}_{folder}({input_type}).json\", 'w') as outfile:\n",
    "    json.dump(new_dataframe, outfile)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
