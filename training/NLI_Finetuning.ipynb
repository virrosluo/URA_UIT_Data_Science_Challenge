{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLI Finetuning Scripts\n",
    "\n",
    "This script is used to Finetune NLI model on Google Colab\n",
    "- Base model: \"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\"\n",
    "- Dataset: public_train_v4.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install datasets\n",
    "!pip install tqdm\n",
    "!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics._classification import _check_targets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter\n",
    "\n",
    "import re\n",
    "\n",
    "import os\n",
    "new_directory = \"drive/MyDrive/FNU/mDeBERTa (ft) V6\"\n",
    "os.chdir(new_directory)\n",
    "print(\"Current Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envir = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') # Check again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\"\n",
    "BATCH_SIZE = 8\n",
    "MODEL_TYPE = ['cls', 'mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(fileName, label2int):\n",
    "    dataframe = pd.read_json(fileName)\n",
    "    data_list = {0:[], 1:[], 2:[]}\n",
    "    for key, values in dataframe.items():\n",
    "        if isinstance(values['evidence'], str):\n",
    "            data_list[ label2int[values['verdict']] ].append( (values['evidence'], values['claim']) )\n",
    "        else:\n",
    "            data_list[ label2int[values['verdict']] ].append( (values['context'].split('.')[0], values['claim']) )\n",
    "            print(f\"Evidence {key} no value !!!\")\n",
    "    return data_list\n",
    "\n",
    "# dataDict: 'sample':[], 'labels':[]\n",
    "def sampling_training_dataset(dataDict:dict):   # Dùng khi sampling dataset đều với các label khác + đảm bảo 1 batch các label sẽ theo order: 0,1,2,0,1,2,0,1,2,...\n",
    "    ros = RandomOverSampler(sampling_strategy='auto', random_state=42)\n",
    "    x_sample, y_sample = ros.fit_resample(dataDict['sample'], dataDict['labels'])\n",
    "    print(f\"Oversampling dataset: {Counter(y_sample)}\")\n",
    "\n",
    "    # Filter by labels\n",
    "    label_sample = {0:[], 1:[], 2:[]}\n",
    "    for sample, label in zip(x_sample, y_sample):\n",
    "        label_sample[label].append(sample)\n",
    "    print(f\"Sorted_dataset: {len(label_sample[0])} -- {len(label_sample[1])} -- {len(label_sample[2])}\")\n",
    "\n",
    "    sorted_dataset = {'sample':[], 'labels':[]}\n",
    "    for first, second, third in zip(label_sample[0], label_sample[1], label_sample[2]):\n",
    "        sorted_dataset['sample'] += [first, second, third]\n",
    "        sorted_dataset['labels'] += [0, 1, 2]\n",
    "    print(f\"Length sample: {len(sorted_dataset['sample'])} -- Length labels: {len(sorted_dataset['labels'])}\")\n",
    "    return sorted_dataset\n",
    "\n",
    "# dataDict: 'sample':[], 'labels':[]\n",
    "def sampling_training_dataset_noEqual(dataDict:dict):   # Dùng khi sampling dataset vẫn không có đều với các label khác\n",
    "    ros = RandomOverSampler(sampling_strategy={2:5000}, random_state=42)\n",
    "    x_sample, y_sample = ros.fit_resample(dataDict['sample'], dataDict['labels'])\n",
    "    print(f\"Oversampling dataset: {Counter(y_sample)}\")\n",
    "\n",
    "    shuffle_dataset = [(sample, label) for sample, label in zip(x_sample, y_sample)]\n",
    "    random.shuffle(shuffle_dataset)\n",
    "\n",
    "    dataset = {'sample':[], 'labels':[]}\n",
    "    for sample, label in shuffle_dataset:\n",
    "        dataset['sample'].append(sample)\n",
    "        dataset['labels'].append(label)\n",
    "    print(f\"Length sample: {len(dataset['sample'])} -- Length labels: {len(dataset['labels'])}\")\n",
    "    return dataset\n",
    "\n",
    "def shuffling_dataset(dataDict:dict):                   # Chỉ shuffle lại data\n",
    "    shuffle_dataset = [(sample, label) for sample, label in zip(dataDict['sample'], dataDict['labels'])]\n",
    "    random.shuffle(shuffle_dataset)\n",
    "\n",
    "    dataset = {'sample':[], 'labels':[]}\n",
    "    for sample, label in shuffle_dataset:\n",
    "        dataset['sample'].append(sample)\n",
    "        dataset['labels'].append(label)\n",
    "    print(f\"Length sample: {len(dataset['sample'])} -- Length labels: {len(dataset['labels'])}\")\n",
    "    return dataset\n",
    "\n",
    "# @dataList: {'0': [(evidence, claim), (evidence, claim), ...]}\n",
    "def separate_dataset(dataList:dict, train_percent:float, validation_percent:float, test_percent:float):\n",
    "    train_set, validation_set, test_set = [{'sample':[], 'labels':[]} for i in range(3)]\n",
    "    for label, sample in dataList.items():\n",
    "        train_x, valid_test_x, train_y, valid_test_y = train_test_split(sample, [label for i in range(len(sample))],\n",
    "                                                                        test_size=(validation_percent + test_percent),\n",
    "                                                                        random_state=42, shuffle=True)\n",
    "\n",
    "        valid_x, test_x, valid_y, test_y = train_test_split(valid_test_x, [label for i in range(len(valid_test_x))],\n",
    "                                                            test_size = test_percent/(test_percent + validation_percent),\n",
    "                                                            random_state = 42, shuffle = True)\n",
    "\n",
    "        train_set['sample'] += train_x\n",
    "        train_set['labels'] += train_y\n",
    "        validation_set['sample'] += valid_x\n",
    "        validation_set['labels'] += valid_y\n",
    "        test_set['sample'] += test_x\n",
    "        test_set['labels'] += test_y\n",
    "\n",
    "        print(f\"{label}: Number of sample in train_set: {len(train_set['sample'])}, valid_set: {len(validation_set['sample'])} and test_set: {len(test_set['sample'])}\")\n",
    "\n",
    "    train_set = sampling_training_dataset(train_set)\n",
    "\n",
    "    return train_set, validation_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2int = {'SUPPORTED':0, 'NEI':1, 'REFUTED':2}\n",
    "raw_dataset = read_dataset(\"public_train_v4.json\", label2int)\n",
    "\n",
    "print(f\"Total dataset: 0:{len(raw_dataset[0])} -- 1:{len(raw_dataset[1])} -- 2:{len(raw_dataset[2])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "train_dict, valid_dict, test_dict = separate_dataset(raw_dataset, train_percent = 0.8, validation_percent = 0.1, test_percent = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict({\n",
    "    'train':Dataset.from_dict(train_dict),\n",
    "    'valid':Dataset.from_dict(valid_dict),\n",
    "    'test':Dataset.from_dict(test_dict)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(text):\n",
    "    text = re.sub(r\"\\n\", \"\", text)\n",
    "    text = text.strip()\n",
    "    text = text[1:].strip() if text[0]=='.' else text\n",
    "    text = text[:-1].strip() if text[-1]=='.' else text\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "def mDeBERTa_tokenize(data): # mDeBERTa model: Taking input_ids\n",
    "    premises = [data_cleaning(premise) for premise, _ in data['sample']]\n",
    "    hypothesis = [data_cleaning(hypothesis) for _, hypothesis in data['sample']]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tokenized_result = tokenizer(premises, hypothesis, return_tensors=\"pt\", padding = True)\n",
    "\n",
    "    return tokenized_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLI_model(nn.Module):\n",
    "    def __init__(self, input_dims, class_weights = torch.tensor([0., 0., 0.])):\n",
    "        super(NLI_model, self).__init__()\n",
    "\n",
    "        self.classification = nn.Sequential(\n",
    "            # # mDeBERTa (ft) V1\n",
    "            # nn.Linear(input_dims, 256),\n",
    "            # nn.Dropout(p=0.1),\n",
    "            # nn.Tanh(),\n",
    "            # nn.Linear(256, 3),\n",
    "            # nn.Dropout(p=0.1)\n",
    "\n",
    "            # # mDeBERTa (ft) V2 V4 V5\n",
    "            # nn.Linear(input_dims, 256),\n",
    "            # nn.Dropout(p=0.1),\n",
    "            # nn.Tanh(),\n",
    "            # nn.Linear(256, 3),\n",
    "\n",
    "            # mDeBERTa (ft) V0 V3 V6\n",
    "            nn.Linear(input_dims, 3)\n",
    "        )\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss(class_weights)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output_linear = self.classification(input)\n",
    "        return output_linear\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx=0):\n",
    "        input_data, targets = train_batch\n",
    "        outputs = self.forward(input_data)\n",
    "        loss = self.criterion(outputs, targets)\n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, batch, batch_idx=0):\n",
    "        input_data, _ = batch\n",
    "        outputs = self.forward(input_data)\n",
    "        prob = outputs.softmax(dim = -1)\n",
    "        sort_prob, sort_indices = torch.sort(-prob, 1)\n",
    "        return sort_indices[:,0], sort_prob[:,0]\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx=0):\n",
    "        _, targets = val_batch\n",
    "        sort_indices, _ = self.predict_step(val_batch, batch_idx)\n",
    "        report = classification_report(list(targets.to('cpu').numpy()), list(sort_indices.to('cpu').numpy()), output_dict=True, zero_division = 1)\n",
    "        return report\n",
    "\n",
    "    def test_step(self, batch, dict_form, batch_idx=0):\n",
    "        _, targets = batch\n",
    "        sort_indices, _ = self.predict_step(batch, batch_idx)\n",
    "        report = classification_report(targets.to('cpu').numpy(), sort_indices.to('cpu').numpy(), output_dict=dict_form, zero_division = 1)\n",
    "        return report\n",
    "\n",
    "    def configure_criterion(self, class_weights):\n",
    "        self.criterion = nn.CrossEntropyLoss(class_weights)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr = 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_phase(LLModel, classifier:NLI_model, valid_loader:DataLoader, process_outputLLModel_fn):\n",
    "# Validation step for testing the model\n",
    "    tensor_list = {'embedding':[], 'label':[]}\n",
    "    with torch.no_grad():\n",
    "        for databatch in valid_loader:\n",
    "            embedding = LLModel(input_ids=databatch['input_ids'].to(envir),\n",
    "                                attention_mask=databatch['attention_mask'].to(envir),\n",
    "                                token_type_ids=databatch['token_type_ids'].to(envir)).last_hidden_state\n",
    "            tensor_list['embedding'].append(process_outputLLModel_fn(embedding))\n",
    "            tensor_list['label'].append(databatch['labels'])\n",
    "        result = classifier.validation_step((torch.concat(tensor_list['embedding'], dim=0).to(envir),\n",
    "                                             torch.concat(tensor_list['label'], dim=0).to(envir)))\n",
    "    return result\n",
    "\n",
    "def testing_phase(LLModel, classifier:NLI_model, test_loader:DataLoader, process_outputLLModel_fn, dict_form):\n",
    "    tensor_list = {'embedding':[], 'label':[]}\n",
    "    with torch.no_grad():\n",
    "        for databatch in test_loader:\n",
    "            embedding = LLModel(input_ids=databatch['input_ids'].to(envir),\n",
    "                                attention_mask=databatch['attention_mask'].to(envir),\n",
    "                                token_type_ids=databatch['token_type_ids'].to(envir)).last_hidden_state\n",
    "            tensor_list['embedding'].append(process_outputLLModel_fn(embedding))\n",
    "            tensor_list['label'].append(databatch['labels'])\n",
    "        result = classifier.test_step((torch.concat(tensor_list['embedding'], dim=0).to(envir),\n",
    "                                       torch.concat(tensor_list['label'], dim=0).to(envir)), dict_form=dict_form)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model(LLModel, classifier:NLI_model, dataset:DatasetDict, process_outputLLModel_fn, checkpoint:int, checkpoint_action, epochs=1000, best_model_func=None):\n",
    "    optimizer = torch.optim.Adam(list(classifier.parameters()) + list(LLModel.parameters()), lr=2e-05, weight_decay=0.01)\n",
    "    record_dict = {'trainLoss':[], 'valAcc':[], 'marco avg f1-score':[]}        # Log training process\n",
    "    if best_model_func != None:\n",
    "        print(\"Has best model criteria\")\n",
    "        best_classifier = None\n",
    "        bestResult = None\n",
    "        best_index = 0\n",
    "\n",
    "    train_loader = DataLoader(dataset['train'], batch_size=BATCH_SIZE, shuffle=False)\n",
    "    valid_loader = DataLoader(dataset['valid'], batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(dataset['test'], batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "# Training step\n",
    "        total_loss = 0\n",
    "        for batch_idx, databatch in enumerate(tqdm(train_loader)):\n",
    "            optimizer.zero_grad()\n",
    "            embedding = LLModel(input_ids=databatch['input_ids'].to(envir),\n",
    "                                attention_mask=databatch['attention_mask'].to(envir),\n",
    "                                token_type_ids=databatch['token_type_ids'].to(envir)).last_hidden_state\n",
    "            processed_embedding = process_outputLLModel_fn(embedding)       # Thực hiện việc xử lý embedding sau khi input qua model (Lấy MEAN hoặc lấy vector CLS)\n",
    "\n",
    "            loss = classifier.training_step((processed_embedding, databatch['labels'].to(envir)))   # Lấy cross entropy qua model classifier\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if batch_idx % checkpoint == 0:\n",
    "                result = validation_phase(LLModel, classifier, valid_loader, process_outputLLModel_fn)  # Đưa model và data qua validation phase\n",
    "                if best_model_func != None:     # Save lại best model bằng cách save checkpoint của LLModel và classifier\n",
    "                    if best_model_func(result, bestResult):\n",
    "                        bestResult = result\n",
    "                        best_classifier = classifier.state_dict()\n",
    "                        LLModel.save_pretrained(f'/content/checkpoint')\n",
    "                        best_index = batch_idx\n",
    "                        print(f\"\\nCheckpoint {batch_idx} -- Accuracy: {result['accuracy']} -- macro_f1: {result['macro avg']['f1-score']} -- loss: {loss.item()} -- best state: {best_index}\")\n",
    "                record_dict['valAcc'].append(result['accuracy'])    # Save eval step\n",
    "                record_dict['marco avg f1-score'].append(result['macro avg']['f1-score'])   # Save eval step\n",
    "                # Save lại eval step vào output file\n",
    "                checkpoint_action(f\"\\nStep {batch_idx + epoch*len(train_loader)} -- Accuracy: {result['accuracy']} -- macro_f1: {result['macro avg']['f1-score']} -- loss: {loss.item()}\")\n",
    "            record_dict['trainLoss'].append(total_loss / (batch_idx + 1))       # Save eval step\n",
    "\n",
    "# Load best_model and infer the test_set\n",
    "    if best_model_func != None:\n",
    "        if best_classifier != None:\n",
    "            print(f\"Load Best Model State Dict: {best_index}\")\n",
    "            classifier.load_state_dict(best_classifier)\n",
    "            LLModel = AutoModel.from_pretrained(\"/content/checkpoint\").to(envir)\n",
    "\n",
    "    result = validation_phase(LLModel, classifier, valid_loader, process_outputLLModel_fn)\n",
    "    print(f\"- Finish --- {input_type} --- Validation Test Accuracy: {result['accuracy']} --- Validation Test F1 Score: {result['macro avg']['f1-score']}\")\n",
    "\n",
    "    result = testing_phase(LLModel, classifier, test_loader, process_outputLLModel_fn, True)\n",
    "    test_acc = result['accuracy']\n",
    "    test_f1score = result['macro avg']['f1-score']\n",
    "\n",
    "    return LLModel, record_dict, test_acc, test_f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = torch.tensor([1., 1., 1.], dtype=torch.float32).to(envir)\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createLLModel(model_name):\n",
    "    model = AutoModel.from_pretrained(model_name).to(envir)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    return model\n",
    "\n",
    "def best_model_func(result, bestResult):\n",
    "    if bestResult == None:\n",
    "        return True\n",
    "    else:\n",
    "        return result['macro avg']['f1-score'] > bestResult['macro avg']['f1-score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict = {i:{'trainLoss':[], 'valAcc':[], 'marco avg f1-score':[]} for i in MODEL_TYPE}\n",
    "for input_type, fn in zip(MODEL_TYPE, [lambda x: x[:, 0, :], lambda x: torch.mean(x[:, 1:, :], dim=1)]):\n",
    "    classifier_model = NLI_model(input_dims=768, class_weights=class_weights).to(envir)\n",
    "    llmodel = createLLModel(MODEL_NAME)\n",
    "    with open(f\"{input_type}_log.txt\", 'w', encoding='utf-8') as log:\n",
    "        llmodel, log_dict[input_type], test_acc, test_f1score = training_model(LLModel=llmodel,\n",
    "                                                                               classifier=classifier_model,\n",
    "                                                                               dataset=mapped_dataset,\n",
    "                                                                               process_outputLLModel_fn=fn,\n",
    "                                                                               checkpoint=100,\n",
    "                                                                               checkpoint_action=lambda x: log.write(x+\"\\n\"),\n",
    "                                                                               epochs=1,\n",
    "                                                                               best_model_func=best_model_func)\n",
    "    print(f'- Finish --- {input_type} --- Test Accuracy: {test_acc} --- Test F1 Score: {test_f1score}')\n",
    "    torch.save({'model_state_dict' : classifier_model.state_dict()}, f\"{input_type}.pt\")    # Save lại classifier\n",
    "    llmodel.save_pretrained(f'mDeBERTa-v3-base-mnli-xnli-{input_type}')         # Save lại best mDeBERTa\n",
    "\n",
    "# Ploting the record\n",
    "fig, axs = plt.subplots(3)  # 2 rows of subplots\n",
    "for method, metrics in log_dict.items():\n",
    "    for idx, (metric_name, record) in enumerate(metrics.items()):\n",
    "        axs[idx].plot(record, label = method)\n",
    "        axs[idx].set_title(metric_name)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig(f\"plot.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Finish pretrained model: {MODEL_NAME}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
